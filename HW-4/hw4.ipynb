{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84814cc2-af81-49c0-b694-5733dc2f1479",
   "metadata": {},
   "source": [
    "# Submission Guidelines\n",
    "\n",
    "## 1. Complete the Jupyter Notebook\n",
    "- Fill in all code sections marked with `TODO` in the skeleton (class `Decision_tree`).  \n",
    "- Implement core functions like `generate_tree`, `combined_metric`, and `metric`.  \n",
    "- The notebook should:\n",
    "  - Train and evaluate the model with both entropy and Gini as splitting metrics.\n",
    "  - Report accuracy for each `min_entropy` value.\n",
    "  - Evaluate different model choices and report results on the test set.\n",
    "\n",
    "## 2. Include All Outputs\n",
    "- Run all cells before submission.  \n",
    "- Validation and test accuracy results should be clearly printed for all settings.  \n",
    "- Optional diagnostics (e.g., confusion matrix, tree depth) must render if included.  \n",
    "- **Missing outputs will result in a penalty.**\n",
    "\n",
    "## 3. Submit the Following:\n",
    "- A zipped folder with:\n",
    "  - Your `.ipynb` notebook (final results)\n",
    "  - Your edited class `Decision_tree` implementation\n",
    "\n",
    "**Do NOT** submit the dataset files, sample notebooks, or saved image files (`.png`).  \n",
    "\n",
    "## 4. Code Clarity & Documentation\n",
    "- Use clear and descriptive variable names.  \n",
    "- Add comments where necessary, especially if you deviate from the skeleton.  \n",
    "- Follow good Python style (PEP-8) for spacing and readability.  \n",
    "\n",
    "⚠️ **Failure to adhere to these guidelines may result in deductions.**\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree for Digit Classification\n",
    "\n",
    "This assignment focuses on implementing a binary-feature decision tree for classifying handwritten digits using the `optdigits` dataset. You will recursively build the tree, compute entropy or Gini metrics, and evaluate performance using validation accuracy.\n",
    "\n",
    "### Rubric (Total = 40 points):\n",
    "\n",
    "---\n",
    "\n",
    "### Tree Implementation – 26 points total\n",
    "\n",
    "**Tree Construction**  \n",
    "- Recursive splitting via `generate_tree(...)` – **5 points**  \n",
    "- Correct use of stopping condition (`min_entropy`) – **4 points**  \n",
    "\n",
    "**Metric Computation**  \n",
    "- `metric(...)` correctly computes entropy – **5 points**  \n",
    "- `combined_metric(...)` uses weighted average – **4 points**\n",
    "\n",
    "**Predict Function**  \n",
    "- `predict(...)` correctly traverses the tree and predicts labels – **4 points**  \n",
    "- Correct feature selection via `select_feature(...)` – **4 points**\n",
    "\n",
    "---\n",
    "\n",
    "### Problem A: Hyperparameter Sweep – 6 points total\n",
    "\n",
    "- Finding and explaining the best `min_entropy ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1, 2.0}` – **3 points**  \n",
    "- Explaining model complexity – **3 points**  \n",
    "\n",
    "---\n",
    "\n",
    "### Problem B: Analysis & Report – 8 points total\n",
    "\n",
    "- Implementing Gini impurity metric – **5 points**  \n",
    "- Compare entropy vs Gini results – **3 points**  \n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- All code to be implemented is marked with `TODO` in the class `Decision_tree`.\n",
    "- Use only binary features in all splits.\n",
    "- You may use external libraries (like `numpy`) but not sklearn's tree implementation.\n",
    "- Be sure to normalize your input only if explicitly instructed — in this task, it is not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc467359-1f19-401b-83fe-c34b6138c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65346ec5-8a80-42f4-af33-c82b8ec1c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_node:\n",
    "    \"\"\"Data structure for nodes in the decision-tree\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature = None      # Index of the selected feature (for non-leaf node)\n",
    "        self.label = None        # Class label (for leaf node), if not leaf node, label will be None\n",
    "        self.left_child = None   # Left child node\n",
    "        self.right_child = None  # Right child node\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "892a141f-aec3-4a36-8a02-b526f700b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_tree:\n",
    "    \"\"\"Decision tree with binary features\"\"\"\n",
    "    \n",
    "    def __init__(self, min_entropy, metric = \"entropy\"):\n",
    "        self.metricname = metric\n",
    "        self.min_entropy = min_entropy\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        # Construct the decision-tree with recursion\n",
    "        self.root = self.generate_tree(train_x,train_y)\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        # Iterate through all samples\n",
    "        prediction = []\n",
    "        for i in range(len(test_x)):\n",
    "            curr_data = test_x[i]\n",
    "            \n",
    "            # Traverse the decision-tree based on the features of the current sample\n",
    "            curr_node = self.root\n",
    "            while True:\n",
    "                if curr_node.label != None:\n",
    "                    break\n",
    "                elif curr_node.feature == None:\n",
    "                    print(\"You haven't selected the feature yet\")\n",
    "                    exit()\n",
    "                else:\n",
    "                    if curr_data[curr_node.feature] == 0:\n",
    "                        curr_node = curr_node.left_child\n",
    "                    else:\n",
    "                        curr_node = curr_node.right_child\n",
    "                        \n",
    "            prediction.append(curr_node.label)\n",
    "        prediction = np.array(prediction)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Use recursion to build up the tree. Starting from the root node, you can call itself \n",
    "    # to determine what is the left_child and what is the right_child. \n",
    "    #\n",
    "    # Return: \n",
    "    #   curr_node: the current tree node you create (Type: Tree_Node)\n",
    "    #\n",
    "    def generate_tree(self, data, label):\n",
    "        # Initialize the current tree node\n",
    "        curr_node = Tree_node()\n",
    "\n",
    "        # Compute the node entropy or gini index and determine if the current node is a \n",
    "        # leaf node. Specifically, if entropy/gini_index (ie, self.metric(label)) < min_entropy), \n",
    "        # determine what will be the label (by choosing the label with the largest count) \n",
    "        # for this leaf node  and directly return the leaf node \n",
    "        node_entropy = self.metric(label)\n",
    "        if node_entropy < self.min_entropy:\n",
    "             # Assign majority class label to the leaf\n",
    "            unique_labels, counts = np.unique(label, return_counts=True)\n",
    "            curr_node.label = unique_labels[np.argmax(counts)]\n",
    "            #\n",
    "            # TODO\n",
    "            #\n",
    "            # Add your lines here\n",
    "\n",
    "            return curr_node\n",
    "\n",
    "        #\n",
    "        # TODO\n",
    "        #\n",
    "        # Select the feature that will best split the current non-leaf node assign the \n",
    "        # feature index to curr_node.feature\n",
    "        #selected_feature = 0  # Placeholder\n",
    "        # Select best feature to split on\n",
    "        selected_feature = self.select_feature(data, label)\n",
    "        curr_node.feature = selected_feature\n",
    "\n",
    "        # Split the data based on the selected feature.\n",
    "        #\n",
    "        # If the selected feature of the data equals to 0, assign the data and corresponding \n",
    "        # point to left_x, left,y otherwise assinged to right_x, right_y\n",
    "        select_x = data[:, selected_feature]\n",
    "        left_x = data[select_x == 0]\n",
    "        left_y = label[select_x == 0]\n",
    "        right_x = data[select_x == 1]\n",
    "        right_y = label[select_x == 1]\n",
    "\n",
    "        #\n",
    "        # TODO\n",
    "        #\n",
    "        # Determine curr_node.left_child and curr_node.right_child by call itself, with left_x, \n",
    "        # left_y and right_x, right_y\n",
    "        # Recursively create subtrees\n",
    "        curr_node.left_child = self.generate_tree(left_x, left_y)\n",
    "        curr_node.right_child = self.generate_tree(right_x, right_y)\n",
    "\n",
    "        return curr_node\n",
    "\n",
    "    # Select the feature that maximize the information gains\n",
    "    #\n",
    "    # Return: \n",
    "    #   best_feat: which is the index of the feature\n",
    "    def select_feature(self, data, label):\n",
    "        best_feat = 0\n",
    "        min_entropy = float(\"inf\")\n",
    "\n",
    "        # Iterate through all features and compute their corresponding entropy \n",
    "        for i in range(len(data[0])):\n",
    "            # Split data based on i-th feature\n",
    "            split_x = data[:, i]\n",
    "            left_y = label[split_x == 0]\n",
    "            right_y = label[split_x == 1]\n",
    "\n",
    "            # Compute the combined entropy which weightedly combine the entropy/gini of \n",
    "            # left_y and right_y\n",
    "            curr_entropy = self.combined_metric(left_y, right_y)\n",
    "\n",
    "            # Select the feature with minimum entropy (set best_feat)\n",
    "            if curr_entropy < min_entropy:\n",
    "                min_entropy = curr_entropy\n",
    "                best_feat = i\n",
    "\n",
    "        return best_feat\n",
    "\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Weightedly combine the entropy/gini of left_y and right_y. The weights are \n",
    "    #\n",
    "    #  [\n",
    "    #    len(left_y) / (len(left_y) + len(right_y)), \n",
    "    #    len(right_y) / (len(left_y) + len(right_y))\n",
    "    #  ] \n",
    "    #\n",
    "    # Return: \n",
    "    #   result\n",
    "    def combined_metric(self,left_y,right_y):\n",
    "        # Compute the entropy of a potential split\n",
    "        #result = 0\n",
    "        \n",
    "        # \n",
    "        # TODO\n",
    "        #\n",
    "        # Add your lines here. Hint: Use self.metric(label) to compute the entropy/gini_index\n",
    "        # Compute the entropy or Gini index of a potential split\n",
    "        total = len(left_y) + len(right_y)\n",
    "        weight_left = len(left_y) / total\n",
    "        weight_right = len(right_y) / total\n",
    "    \n",
    "        # Use self.metric() to compute impurity of each side and combine\n",
    "        result = weight_left * self.metric(left_y) + weight_right * self.metric(right_y)\n",
    "        return result\n",
    "\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Compute entropy/gini_index based on the labels. The entropy is\n",
    "    # \n",
    "    #   entropy = -sum_i p_i * log2(p_i + 1e-15) \n",
    "    #\n",
    "    # Where we add 1e-15 inside the log when computing the entropy to prevent numerical \n",
    "    # issues. The Gini Index is \n",
    "    # \n",
    "    #   gini_index = 1 - sum_i p_i^2\n",
    "    #\n",
    "    def metric(self, label):\n",
    "        result = 0\n",
    "        if self.metricname == \"entropy\":\n",
    "            class_label, count = np.unique(label, return_counts = True)\n",
    "            count = count / len(label)\n",
    "        \n",
    "            # TODO\n",
    "            # Add a line here\n",
    "            #Calculate entropy\n",
    "            result = -np.sum(count * np.log2(count + 1e-15))\n",
    "            \n",
    "        elif self.metricname == \"gini_index\":\n",
    "            class_label, count = np.unique(label, return_counts = True)\n",
    "            count = count / len(label)\n",
    "            \n",
    "            # \n",
    "            # TODO\n",
    "            #calcualte gini index\n",
    "            # Add a line here\n",
    "            result = 1 - np.sum(count ** 2)\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662802b0-e0a5-4138-a605-0728a78ae7fa",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f6969e1-b6d9-48ae-99ff-31e799765064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (3000, 64)\n",
      "train_y: (3000,)\n",
      "valid_x: (562, 64)\n",
      "valid_y: (562,)\n",
      "test_x: (562, 64)\n",
      "test_y: (562,)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "train_data = np.genfromtxt(\"optdigits_train.txt\", delimiter = \",\")\n",
    "train_x = train_data[:, :-1]\n",
    "train_y = train_data[:, -1].astype(\"int\")\n",
    "\n",
    "# Validation data\n",
    "valid_data = np.genfromtxt(\"optdigits_valid.txt\", delimiter = \",\")\n",
    "valid_x = valid_data[:, :-1]\n",
    "valid_y = valid_data[:, -1].astype(\"int\")\n",
    "\n",
    "# Test data\n",
    "test_data = np.genfromtxt(\"optdigits_test.txt\", delimiter = \",\")\n",
    "test_x = test_data[:, :-1]\n",
    "test_y = test_data[:, -1].astype(\"int\")\n",
    "\n",
    "print(\"train_x:\", train_x.shape)\n",
    "print(\"train_y:\", train_y.shape)\n",
    "print(\"valid_x:\", valid_x.shape)\n",
    "print(\"valid_y:\", valid_y.shape)\n",
    "print(\"test_x:\",  test_x.shape)\n",
    "print(\"test_y:\",  test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930ecba-8fc0-4a00-a505-e768e0899fb7",
   "metadata": {},
   "source": [
    "## Problem A\n",
    "\n",
    "Implement a Decision Tree with the minimum node entropy $\\theta$=0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0 and 4.0. The minimum node entropy is used to determine the leaf nodes, i.e., a node is a leaf node if its node entropy is lower than the selected minimum node entropy.\n",
    "\n",
    "Report the training and validation accuracy by different $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ce646c3-0708-44b4-9a9b-004ad95e550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/validation accuracy for minimum node entropy 0.01 is 1.000 / 0.8629893238434164\n",
      "Training/validation accuracy for minimum node entropy 0.05 is 0.999 / 0.8629893238434164\n",
      "Training/validation accuracy for minimum node entropy 0.1 is 0.997 / 0.8647686832740213\n",
      "Training/validation accuracy for minimum node entropy 0.2 is 0.990 / 0.8665480427046264\n",
      "Training/validation accuracy for minimum node entropy 0.5 is 0.963 / 0.8629893238434164\n",
      "Training/validation accuracy for minimum node entropy 0.8 is 0.919 / 0.8558718861209964\n",
      "Training/validation accuracy for minimum node entropy 1 is 0.871 / 0.8398576512455516\n",
      "Training/validation accuracy for minimum node entropy 2.0 is 0.596 / 0.599644128113879\n",
      "Test accuracy with minimum node entropy 0.2 is 0.872\n"
     ]
    }
   ],
   "source": [
    "# Problem A\n",
    "\n",
    "# Experiment with different settings of minimum node entropy\n",
    "candidate_min_entropy = [0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1, 2.0]\n",
    "\n",
    "valid_accuracy = []\n",
    "for i, min_entropy in enumerate(candidate_min_entropy):\n",
    "    # Initialize and fit.\n",
    "    clf = Decision_tree(min_entropy = min_entropy, metric = \"entropy\")\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    predictions_train = clf.predict(train_x)\n",
    "    predictions_val = clf.predict(valid_x)\n",
    "    \n",
    "    curr_train_accuracy = np.count_nonzero(predictions_train.reshape(-1) == train_y.reshape(-1)) / len(train_x)\n",
    "    curr_valid_accuracy = np.count_nonzero(predictions_val.reshape(-1) == valid_y.reshape(-1)) / len(valid_x)\n",
    "    valid_accuracy.append(curr_valid_accuracy)\n",
    "    \n",
    "    print(\n",
    "        f\"Training/validation accuracy for minimum node entropy {candidate_min_entropy[i]} \"\n",
    "        f\"is {curr_train_accuracy:.3f} / {curr_valid_accuracy}\"\n",
    "    )\n",
    "\n",
    "# Select the best minimum node entropy and use it to train the model\n",
    "best_entropy = candidate_min_entropy[np.argmax(valid_accuracy)]\n",
    "clf = Decision_tree(min_entropy = best_entropy)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate on test data\n",
    "predictions = clf.predict(test_x)\n",
    "accuracy = np.count_nonzero(predictions.reshape(-1) == test_y.reshape(-1)) / len(test_x)\n",
    "\n",
    "print(f\"Test accuracy with minimum node entropy {best_entropy} is {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fc159-6277-4ff2-9e7e-8a4e7dfca0a8",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Which $\\theta$ performs best? Report the accuracy on the test set using this $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270313c5-68b7-460a-9ca7-d559b9e2e00e",
   "metadata": {},
   "source": [
    "[ANSWER GOES HERE]\n",
    "The best performing model is the decision tree trained with a minimum node entropy of 0.2. The test accuracy using this is 0.872. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a503e77-26b3-4953-8059-0b62732fd7bd",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "What can you say about the model complexity of the Decision Tree, given the training and validation accuracy? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb8c7d-7ea0-4787-b6ff-bd994e4ff4d7",
   "metadata": {},
   "source": [
    "[ANSWER GOES HERE]\n",
    "When the minimum entropy is very low(0.01), the model has very high training accuracy (1.0). However, the validation accuracy is low (0.862) resulting in overfitting. it means that the tree is too complex and is also learning the noise in the dataset. As the minimum entropy starts increasing the training acccuracy starts decreasing slightly and the validation accuracy starts increasing. At the entropy of 2.0, the model underfits the dataset. The entropy of 0.2 gives similar accuracy(0.59) in training and validation set and also the good test accuracy balancing the model complexity and the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebeb784-0a2a-4a5c-87b0-2637164fb0b8",
   "metadata": {},
   "source": [
    "## Problem B\n",
    "\n",
    "Apart from the entropy, another category of Decision Tree algorithm leverages a different metric: Gini Index. The following is the formula of Gini Index: \n",
    "       \\begin{align}\n",
    "           Gini = 1 - \\sum_i p_i^2,\n",
    "       \\end{align}\n",
    "       where $p_i$ is the sample as the $p_i$ in entropy that denotes probability of class $i$. You are going to implement *gini\\_index()*, and run the same set of experiments again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c20a285a-ae71-419d-a7a6-0699aba402b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/validation accuracy for minimum gini_index 0.01 is 0.999 / 0.8469750889679716\n",
      "Training/validation accuracy for minimum gini_index 0.05 is 0.990 / 0.8523131672597865\n",
      "Training/validation accuracy for minimum gini_index 0.1 is 0.978 / 0.8505338078291815\n",
      "Training/validation accuracy for minimum gini_index 0.2 is 0.948 / 0.8451957295373665\n",
      "Training/validation accuracy for minimum gini_index 0.5 is 0.800 / 0.7669039145907474\n",
      "Training/validation accuracy for minimum gini_index 0.8 is 0.384 / 0.39501779359430605\n",
      "Training/validation accuracy for minimum gini_index 1 is 0.100 / 0.11743772241992882\n",
      "Training/validation accuracy for minimum gini_index 2.0 is 0.100 / 0.11743772241992882\n",
      "Test accuracy with minimum gini_index 0.05 is 0.867\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different settings of minimum node entropy\n",
    "candidate_min_entropy = [0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1, 2.0]\n",
    "\n",
    "valid_accuracy = []\n",
    "for i, min_entropy in enumerate(candidate_min_entropy):\n",
    "    # Initialize and fit.\n",
    "    clf = Decision_tree(min_entropy = min_entropy, metric = \"gini_index\")\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    predictions_train = clf.predict(train_x)\n",
    "    predictions_val = clf.predict(valid_x)\n",
    "    \n",
    "    curr_train_accuracy = np.count_nonzero(predictions_train.reshape(-1) == train_y.reshape(-1)) / len(train_x)\n",
    "    curr_valid_accuracy = np.count_nonzero(predictions_val.reshape(-1) == valid_y.reshape(-1)) / len(valid_x)\n",
    "    valid_accuracy.append(curr_valid_accuracy)\n",
    "    \n",
    "    print(\n",
    "        f\"Training/validation accuracy for minimum gini_index {candidate_min_entropy[i]} \"\n",
    "        f\"is {curr_train_accuracy:.3f} / {curr_valid_accuracy}\"\n",
    "    )\n",
    "\n",
    "# Select the best minimum node entropy and use it to train the model\n",
    "best_entropy = candidate_min_entropy[np.argmax(valid_accuracy)]\n",
    "clf = Decision_tree(min_entropy = best_entropy)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate on test data\n",
    "predictions = clf.predict(test_x)\n",
    "accuracy = np.count_nonzero(predictions.reshape(-1) == test_y.reshape(-1)) / len(test_x)\n",
    "\n",
    "print(f\"Test accuracy with minimum gini_index {best_entropy} is {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9eb562-4ae0-4e8a-84b8-29386f074a8f",
   "metadata": {},
   "source": [
    "#### Question\n",
    "What can you say about the performance? Are they more or less the same, or which one is better. Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749b15d-f4ad-44b0-8bef-8054726aa0fd",
   "metadata": {},
   "source": [
    "[ANSWER GOES HERE]\n",
    "What i observed is across all the minimum entropy values, the entropy measure generally has the higher training accuracy as compared to the gini index. In case of the validation accuracy, entropy achives slightly better accuracy. eg at 0.05 minimum entroy, the validation accuracy is 0.862, while at minimum gini index of 0.05, the validation accuracy is 0.852. In case of test accuracy, entropy achieves similar but slightly better test accuracy (0.872 at min_entropy = 0.2) compared to Gini index (0.867 at minimum gini_index of 0.05). As minimum entropy or minimum gini_index increases, both training and validation accuracy drop significantly, indicating underfitting.\n",
    "Overall, the performance of both entropy and gini index is comparable. Entropy performs slightly better than Gini index in terms of validation and test accuracy, indicating it may create splits that generalize better for this dataset. However, the difference in accuracy is not that drastic. The choice between these would depend on the nature of question/dataset and computational efficiency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb21931-b3c9-4eb8-8f9d-dc0da5a86f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
